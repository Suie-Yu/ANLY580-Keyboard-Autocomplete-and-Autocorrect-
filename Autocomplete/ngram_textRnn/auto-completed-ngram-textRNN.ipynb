{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import math\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "from pathlib import Path\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import matplotlib\n",
    "# matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./wiki_100000.csv')\n",
    "df = df[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['text'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./train.txt','w',encoding='utf-8')\n",
    "f1 = open('./test.txt','w',encoding='utf-8')\n",
    "f2= open('./valid.txt','w',encoding='utf-8')\n",
    "for index,value in enumerate(df['text'].values):\n",
    "    value = value.strip()\n",
    "    if value:\n",
    "        if index < 50:\n",
    "            f.write(value + '\\n')\n",
    "        elif index > 75:\n",
    "            f1.write(value + '\\n')\n",
    "        else :  \n",
    "            f2.write(value + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class n_gram(nn.Module):\n",
    "    def __init__(self,n_class ,n_step,emb_size):\n",
    "        super(n_gram, self).__init__()\n",
    "#         pdb.set_trace()\n",
    "        self.embed = nn.Embedding(n_class, emb_size)   # (vocab_size,n_dim)\n",
    "        self.classify = nn.Sequential(\n",
    "            nn.Linear(n_step * emb_size, 128),   \n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, n_class)\n",
    "        )\n",
    " \n",
    "    def forward(self, x):\n",
    "        voc_embed = self.embed(x)  # get word embedding context_size*n_dim\n",
    "        voc_embed = voc_embed.view(1, -1)  # join two word vectors together  1*(context_size*n_dim)\n",
    "        out = self.classify(voc_embed)   # 1*vocab_size\n",
    "        return out\n",
    "class NNLM(nn.Module):\n",
    "    def __init__(self, n_class,n_step,emb_size,n_hidden):\n",
    "        super(NNLM, self).__init__()\n",
    "        self.n_step = n_step\n",
    "        self.emb_size = emb_size\n",
    "        self.n_hidden = n_hidden\n",
    "\n",
    "        self.C = nn.Embedding(n_class, emb_size)\n",
    "        self.w1 = nn.Linear(n_step * emb_size, n_hidden, bias=False)\n",
    "        self.b1 = nn.Parameter(torch.ones(n_hidden))\n",
    "        self.w2 = nn.Linear(n_hidden, n_class, bias=False)\n",
    "        self.w3 = nn.Linear(n_step * emb_size, n_class, bias=False)\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.C(X)\n",
    "        X = X.view(-1, self.n_step * self.emb_size)\n",
    "        Y1 = torch.tanh(self.b1 + self.w1(X))\n",
    "        b2 = self.w3(X)\n",
    "        Y2 = b2 + self.w2(Y1)\n",
    "        return Y2\n",
    "\n",
    "class TextRNN(nn.Module):\n",
    "    def __init__(self, n_class,emb_size,n_hidden):\n",
    "        super(TextRNN, self).__init__()\n",
    "        self.C = nn.Embedding(n_class, embedding_dim=emb_size)\n",
    "        self.rnn = nn.RNN(input_size=emb_size, hidden_size=n_hidden)\n",
    "        self.W = nn.Linear(n_hidden, n_class, bias=False)\n",
    "        self.b = nn.Parameter(torch.ones([n_class]))\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.C(X)\n",
    "        X = X.transpose(0, 1)\n",
    "        outputs, hidden = self.rnn(X)\n",
    "        outputs = outputs[-1] \n",
    "        model = self.W(outputs) + self.b\n",
    "        return model\n",
    "\n",
    "class TextRNN_attention(nn.Module):\n",
    "    def __init__(self, n_class,emb_size,n_hidden):\n",
    "        super(TextRNN_attention, self).__init__()\n",
    "        \n",
    "        self.C = nn.Embedding(n_class, embedding_dim=emb_size)\n",
    "        self.rnn = nn.RNN(input_size=emb_size, hidden_size=n_hidden)\n",
    "        self.W = nn.Linear(2 * n_hidden, n_class, bias=False)\n",
    "        self.b = nn.Parameter(torch.ones([n_class]))\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.C(X)\n",
    "        X = X.transpose(0, 1)  # X: [n_step, batch_size, embeding size]\n",
    "        outputs, hidden = self.rnn(X)\n",
    "        output = outputs[-1]\n",
    "        attention = []\n",
    "        for it in outputs[:-1]:\n",
    "            attention.append(torch.mul(it, output).sum(dim=1).tolist())\n",
    "        attention = torch.tensor(attention)\n",
    "        attention = attention.transpose(0, 1)\n",
    "        attention = nn.functional.softmax(attention, dim=1).transpose(0, 1)\n",
    "        # get soft attention\n",
    "        attention_output = torch.zeros(outputs.size()[1], n_hidden)\n",
    "        for i in range(outputs.size()[0] - 1):\n",
    "            attention_output += torch.mul(attention[i], outputs[i].transpose(0, 1)).transpose(0, 1)\n",
    "        output = torch.cat((attention_output, output), 1)\n",
    "        model = torch.mm(output, self.W.weight.T) + self.b  #[batch_size, n_class]\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_epoch(model, loss_function, optimizer, data_loader, device, epoch):\n",
    "    model.train()\n",
    "    accu_loss = torch.zeros(1).to(device) \n",
    "    optimizer.zero_grad()\n",
    "    data_loader = tqdm(data_loader, file=sys.stdout)\n",
    "    preds_all = []\n",
    "    labels_all = []\n",
    "    \n",
    "    for step, data in enumerate(data_loader):\n",
    "        input, target = data\n",
    "        pred = model(input.to(device))\n",
    "        loss = loss_function(pred, target.to(device))\n",
    "        loss.backward()\n",
    "        accu_loss += loss.detach()\n",
    "        data_loader.desc = \"[train epoch {}] loss: {:.3f} ppl: {:.3f} \".format(\n",
    "            epoch,accu_loss.item() / (step + 1),math.exp(accu_loss.item() / (step + 1))\n",
    "        )\n",
    "        if not torch.isfinite(loss):\n",
    "            print('WARNING: non-finite loss, ending training ', loss)\n",
    "            sys.exit(1)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    return accu_loss.item() / (step + 1),math.exp(accu_loss.item() / (step + 1))\n",
    "\n",
    "def evaluate(model, loss_function, data_loader, device, epoch):\n",
    "    model.eval()\n",
    "\n",
    "    accu_loss = torch.zeros(1).to(device)  # cumulative loss\n",
    "    preds_all = []\n",
    "    labels_all = []\n",
    "    data_loader = tqdm(data_loader, file=sys.stdout)\n",
    "    for step, data in enumerate(data_loader):\n",
    "        input, target = data\n",
    "        pred = model(input.to(device))\n",
    "        loss = loss_function(pred, target.to(device))\n",
    "        accu_loss += loss\n",
    "        data_loader.desc = \"[valid epoch {}] loss: {:.3f} ppl: {:.3f} \".format(\n",
    "            epoch,\n",
    "            accu_loss.item() / (step + 1),math.exp(accu_loss.item() / (step + 1))\n",
    "        )\n",
    "    return accu_loss.item() / (step + 1), math.exp(accu_loss.item() / (step + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataSet(Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        input = self.inputs[item]\n",
    "        target = self.targets[item]\n",
    "\n",
    "        return input, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = './train.txt'\n",
    "valid_path = './valid.txt'\n",
    "test_path = './test.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A piece of text processed into words\n",
    "train_words_path = './train_alpha.txt'#Processed as each line 'l i k e #' is written to the file\n",
    "valid_words_path = './valid_alpha.txt'\n",
    "test_words_path = './test_alpha.txt'\n",
    "def process_words(origin_path, output_path):\n",
    "    f = open(origin_path, 'r',encoding='utf-8')\n",
    "    f_out = open(output_path,'w',encoding='utf-8')\n",
    "    lines = f.readlines()\n",
    "    word_list = []\n",
    "    for sentence in lines:\n",
    "        sentence = sentence.strip()\n",
    "        if sentence == '':\n",
    "            continue\n",
    "        word_list += sentence.split()\n",
    "    word_list = set(word_list)\n",
    "    for word in word_list:\n",
    "        if word.isalpha() and len(list(word)) > 4:\n",
    "            f_out.write( ' '.join(list(word))+ ' #' + '\\n')\n",
    "process_words(train_path,train_words_path)\n",
    "process_words(valid_path,valid_words_path)\n",
    "process_words(test_path,test_words_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vocab(data_path):\n",
    "        word_list = []\n",
    "        f = open(data_path, 'r',encoding='utf-8')\n",
    "        lines = f.readlines()\n",
    "        for sentence in lines:\n",
    "            word_list += sentence.split()\n",
    "        word_list = list(set(word_list))\n",
    "        word2index_dict = {w: i + 2 for i, w in enumerate(word_list)}\n",
    "        word2index_dict['<PAD>'] = 0\n",
    "        word2index_dict['<UNK>'] = 1\n",
    "        word2index_dict = dict(sorted(word2index_dict.items(), key=lambda x: x[1]))  # sort\n",
    "        index2word_dict = {index: word for word, index in word2index_dict.items()}\n",
    "        json_str = json.dumps(word2index_dict, indent=4)\n",
    "        with open(vocab_path, 'w') as json_file:\n",
    "            json_file.write(json_str)\n",
    "\n",
    "        return word2index_dict, index2word_dict\n",
    "\n",
    "def generate_dataset(train_path, word2index_dict, n_step):\n",
    "    def word2index(word):\n",
    "        try:\n",
    "            return word2index_dict[word]\n",
    "        except:\n",
    "            return 1\n",
    "    input_list = []\n",
    "    target_list = []\n",
    "    f = open(train_path, 'r',encoding='utf-8')\n",
    "    lines = f.readlines()\n",
    "    for sentence in lines:\n",
    "        sentence = sentence.strip()\n",
    "        if sentence == '':\n",
    "#             pdb.set_trace()\n",
    "            continue\n",
    "        word_list = sentence.split()\n",
    "        if len(word_list) < n_step + 1: \n",
    "            word_list = ['<PAD>'] * (n_step + 1 - len(word_list)) + word_list\n",
    "        index_list = [word2index(word) for word in word_list]\n",
    "        for i in range(len(word_list) - n_step):\n",
    "            input = index_list[i: i + n_step]\n",
    "            target = index_list[i + n_step]\n",
    "\n",
    "            input_list.append(torch.tensor(input))\n",
    "            target_list.append(torch.tensor(target))\n",
    "    dataset = MyDataSet(input_list, target_list)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class trainer():\n",
    "    def __init__(self,n_step,epochs,train_path,valid_path,test_path,train_loader,test_loader,valid_loader,word2index_dict,index2word_dict,model_dic):\n",
    "        self.epochs = epochs\n",
    "        self.n_step = n_step\n",
    "        self.n_hidden = 5  \n",
    "        self.emb_size = 512 \n",
    "        self.save_epoch = 1\n",
    "        self.batch_size = 1\n",
    "        self.lr = 1e-3\n",
    "        self.train_path = train_path\n",
    "        self.valid_path = valid_path\n",
    "        self.test_path = test_path\n",
    "        self.word2index_dict = word2index_dict\n",
    "        self.index2word_dict = index2word_dict\n",
    "        self.n_class = len(self.word2index_dict)\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.valid_loader = valid_loader\n",
    "        self.model_dic = model_dic\n",
    "    def train_test_model(self,modelname): \n",
    "        models_path = './' + modelname\n",
    "        if not os.path.exists(models_path):\n",
    "            os.makedirs(models_path)\n",
    "        model = self.model_dic[modelname].to(device)\n",
    "        loss_function = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=self.lr)\n",
    "        trains_loss = []\n",
    "        trains_ppl = []\n",
    "        valids_ppl = []\n",
    "        valids_loss = []\n",
    "        for epoch in range(self.epochs):\n",
    "            train_loss,train_ppl= train_on_epoch(model=model,loss_function=loss_function,optimizer=optimizer,data_loader=self.train_loader,device=device,epoch=epoch)\n",
    "            valid_loss,valid_ppl = evaluate(model=model,loss_function=loss_function,data_loader=self.valid_loader,device=device,epoch=epoch)\n",
    "            trains_loss.append(train_loss)\n",
    "            trains_ppl.append(train_ppl)\n",
    "            valids_loss.append(valid_loss)\n",
    "            valids_ppl.append(valid_ppl)\n",
    "            if (epoch + 1) % self.save_epoch == 0:\n",
    "                torch.save(model, os.path.join(models_path, f'weights-{epoch + 1}.ckpt'))\n",
    "        test_loss,test_ppl = evaluate(model=model,loss_function=loss_function,data_loader=self.test_loader,device=device,epoch=epoch)\n",
    "        return test_loss,test_ppl,trains_loss,valids_loss,trains_ppl,valids_ppl\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_loss(epochs,trains_loss,valids_loss,models_path):\n",
    "    plt.plot( trains_loss, 'k--', label='train_loss', lw=2,color='g')\n",
    "    plt.plot(valids_loss, 'k--', label='valid_loss', lw=2,color='r')\n",
    "    plt.xlim([0,epochs])  \n",
    "    plt.ylim([0, max(trains_loss)])\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')  \n",
    "    plt.title('loss_curve')\n",
    "    plt.savefig(models_path + '/loss.png')\n",
    "    plt.cla()\n",
    "    plt.clf()\n",
    "#     plt.show()\n",
    "    plt.close()\n",
    "#     gc.collect()\n",
    "def draw_ppl(epochs,trains_ppl,valids_ppl,models_path):\n",
    "    plt.plot([i for i in range(epochs)], trains_ppl, 'k--', label='train_ppl', lw=2,color='g')\n",
    "    plt.plot([i for i in range(epochs)], valids_ppl, 'k--', label='valid_ppl', lw=2,color='r')\n",
    "    plt.xlim([0,epochs])  \n",
    "    plt.ylim([0, max(trains_ppl)])\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('ppl')  \n",
    "    plt.title('ppl_curve')\n",
    "    plt.savefig(models_path + '/ppl.png')\n",
    "    plt.cla()\n",
    "    plt.clf()\n",
    "    plt.close()\n",
    "#     gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset_one(sentence, word2index_dict, n_step=5):\n",
    "    def word2index(word):\n",
    "        try:\n",
    "            return word2index_dict[word]\n",
    "        except:\n",
    "            return 1  # <UNK>\n",
    "    input_list = []\n",
    "    target_list = []\n",
    "    word_list = sentence.split()\n",
    "    if len(word_list) < n_step + 1:  # Insufficient words in the sentence, padding\n",
    "        word_list = ['<PAD>'] * (n_step + 1 - len(word_list)) + word_list\n",
    "    index_list = [word2index(word) for word in word_list]\n",
    "    input = index_list[-n_step: ]\n",
    "    target = index_list[n_step]\n",
    "    input_list.append(torch.tensor(input))\n",
    "    target_list.append(torch.tensor(target))\n",
    "    dataset = MyDataSet(input_list, target_list)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_path = './vocab.json'\n",
    "word2index_dict, index2word_dict = generate_vocab(train_words_path)\n",
    "n_class = len(word2index_dict)  \n",
    "# instantiated dataset\n",
    "n_step = 2  # number of steps, n-1 in paper\n",
    "emb_size = 512\n",
    "n_hidden = 5\n",
    "epochs = 3\n",
    "batch_size = 1\n",
    "save_epoch = 1  # Save the model every epoch\n",
    "nw = min([os.cpu_count(), batch_size if  batch_size > 1 else 0, 8]) \n",
    "train_dataset = generate_dataset(train_words_path, word2index_dict, n_step)\n",
    "valid_dataset = generate_dataset(valid_words_path, word2index_dict, n_step)\n",
    "test_dataset = generate_dataset(test_words_path, word2index_dict, n_step)\n",
    "train_loader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True, pin_memory=True,num_workers=nw)\n",
    "valid_loader = DataLoader(valid_dataset,batch_size=batch_size,shuffle=True,pin_memory=True,num_workers=nw)\n",
    "test_loader = DataLoader(test_dataset,batch_size=batch_size,shuffle=True,pin_memory=True,num_workers=nw)\n",
    "model_dic = {\"ngram_alpha\":n_gram(n_class,n_step,emb_size), \"TextRnn_alpha\":TextRNN(n_class,emb_size,n_hidden),  \"TextRnnAttention_alpha\":TextRNN_attention(n_class,emb_size,n_hidden),\"NNLM_alpha\":NNLM(n_class,n_step,emb_size,n_hidden)}\n",
    "mm1 = trainer(n_step,epochs,train_path,valid_path,test_path,train_loader,test_loader,valid_loader,word2index_dict,index2word_dict,model_dic)\n",
    "test_loss_models_alpha = {}\n",
    "test_ppl_models_alpha = {}\n",
    "def main_alpha(model_name):\n",
    "    models_path = './' + model_name\n",
    "    test_loss, test_ppl,trains_loss,valids_loss,trains_ppl,valids_ppl = mm1.train_test_model(model_name)\n",
    "    f_loss = open(models_path + '/trian_valid_loss.txt','w',encoding='utf-8')\n",
    "    \n",
    "    f_ppl = open(models_path + '/trian_valid_ppl.txt','w',encoding='utf-8')\n",
    "    \n",
    "    for loss_t,loss_v in zip(trains_loss,valids_loss):\n",
    "        f_loss.write(str(loss_t) + '\\t' + str(loss_v) + '\\n')\n",
    "        \n",
    "    for ppl_t,ppl_v in zip(trains_loss,valids_loss):\n",
    "        f_ppl.write(str(ppl_t) + '\\t' + str(ppl_v) + '\\n')\n",
    "#     draw_loss(epochs,trains_loss,valids_loss,models_path)\n",
    "#     draw_ppl(epochs1,trains_ppl,valids_ppl,models_path)\n",
    "    test_loss_models_alpha[model_name] = test_loss\n",
    "    test_ppl_models_alpha[model_name] = test_ppl\n",
    "# word completion\n",
    "def next_alpha_predict(test_model,sentence):\n",
    "    model = torch.load(test_model).to(device)\n",
    "    with open(vocab_path, \"r\") as f:\n",
    "        word2index_dict = json.load(f)\n",
    "    index2word_dict = {index: word for word, index in word2index_dict.items()}\n",
    "    print('Enter letters: {}'.format(sentence))\n",
    "    while sentence[-1] != '#':\n",
    "        sentence_dataset = generate_dataset_one(sentence,word2index_dict=word2index_dict)\n",
    "        data_loader = DataLoader(sentence_dataset,batch_size=1,shuffle=False,pin_memory=True,num_workers=nw)\n",
    "        for step, data in enumerate(data_loader):\n",
    "            input, target = data\n",
    "            pred = model(input.to(device))\n",
    "            preds = pred.data.max(1, keepdim=True)[1]\n",
    "            sentence = sentence + ' ' + index2word_dict[preds.item()]\n",
    "    print('Word completion： {}'.format(sentence))\n",
    "\n",
    "# Word completion predicts the next candidate letter\n",
    "def rank_alpha_predict(test_model,sentence):\n",
    "    model = torch.load(test_model).to(device)\n",
    "    with open(vocab_path, \"r\") as f:\n",
    "            word2index_dict = json.load(f)\n",
    "    index2word_dict = {index: word for word, index in word2index_dict.items()}\n",
    "    sentence_dataset = generate_dataset_one(sentence,word2index_dict=word2index_dict)\n",
    "    data_loader = DataLoader(sentence_dataset,batch_size=1,shuffle=False,pin_memory=True,num_workers=nw)\n",
    "    for step, data in enumerate(data_loader):\n",
    "        input, target = data\n",
    "        pred = model(input.to(device))\n",
    "        rank_ids = torch.argsort(pred,dim=1,descending=True)[:,:5].numpy()[0]\n",
    "        rank_words = [index2word_dict[i] for i in rank_ids]\n",
    "        print('Next letter candidate: {}'.format(rank_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train epoch 0] loss: 2.931 ppl: 18.737 : 100%|█████████████████████████████████| 45100/45100 [02:35<00:00, 290.29it/s]\n",
      "[valid epoch 0] loss: 2.561 ppl: 12.944 : 100%|████████████████████████████████| 33418/33418 [00:11<00:00, 2861.03it/s]\n",
      "[train epoch 1] loss: 2.503 ppl: 12.217 : 100%|█████████████████████████████████| 45100/45100 [02:34<00:00, 291.54it/s]\n",
      "[valid epoch 1] loss: 2.434 ppl: 11.400 : 100%|████████████████████████████████| 33418/33418 [00:12<00:00, 2740.05it/s]\n",
      "[train epoch 2] loss: 2.421 ppl: 11.259 : 100%|█████████████████████████████████| 45100/45100 [02:32<00:00, 295.65it/s]\n",
      "[valid epoch 2] loss: 2.406 ppl: 11.087 : 100%|████████████████████████████████| 33418/33418 [00:11<00:00, 2859.71it/s]\n",
      "[valid epoch 2] loss: 2.416 ppl: 11.204 : 100%|████████████████████████████████| 28570/28570 [00:10<00:00, 2807.30it/s]\n",
      "[train epoch 0] loss: 2.623 ppl: 13.770 : 100%|█████████████████████████████████| 45100/45100 [04:00<00:00, 187.55it/s]\n",
      "[valid epoch 0] loss: 2.642 ppl: 14.038 : 100%|████████████████████████████████| 33418/33418 [00:10<00:00, 3146.62it/s]\n",
      "[train epoch 1] loss: 2.670 ppl: 14.434 : 100%|█████████████████████████████████| 45100/45100 [04:55<00:00, 152.74it/s]\n",
      "[valid epoch 1] loss: 2.812 ppl: 16.635 : 100%|████████████████████████████████| 33418/33418 [00:10<00:00, 3112.12it/s]\n",
      "[train epoch 2] loss: 2.745 ppl: 15.564 : 100%|█████████████████████████████████| 45100/45100 [05:21<00:00, 140.31it/s]\n",
      "[valid epoch 2] loss: 2.769 ppl: 15.944 : 100%|████████████████████████████████| 33418/33418 [00:10<00:00, 3124.31it/s]\n",
      "[valid epoch 2] loss: 2.766 ppl: 15.892 : 100%|████████████████████████████████| 28570/28570 [00:09<00:00, 3014.08it/s]\n",
      "[train epoch 0] loss: 2.704 ppl: 14.937 : 100%|█████████████████████████████████| 45100/45100 [02:21<00:00, 317.82it/s]\n",
      "[valid epoch 0] loss: 2.597 ppl: 13.424 : 100%|████████████████████████████████| 33418/33418 [00:18<00:00, 1842.24it/s]\n",
      "[train epoch 1] loss: 2.605 ppl: 13.529 : 100%|█████████████████████████████████| 45100/45100 [02:44<00:00, 273.73it/s]\n",
      "[valid epoch 1] loss: 2.601 ppl: 13.478 : 100%|████████████████████████████████| 33418/33418 [00:18<00:00, 1778.12it/s]\n",
      "[train epoch 2] loss: 2.612 ppl: 13.625 : 100%|█████████████████████████████████| 45100/45100 [02:54<00:00, 257.85it/s]\n",
      "[valid epoch 2] loss: 2.610 ppl: 13.600 : 100%|████████████████████████████████| 33418/33418 [00:19<00:00, 1731.96it/s]\n",
      "[valid epoch 2] loss: 2.610 ppl: 13.596 : 100%|████████████████████████████████| 28570/28570 [00:15<00:00, 1861.42it/s]\n",
      "[train epoch 0] loss: 2.681 ppl: 14.604 : 100%|█████████████████████████████████| 45100/45100 [02:37<00:00, 286.15it/s]\n",
      "[valid epoch 0] loss: 2.563 ppl: 12.971 : 100%|████████████████████████████████| 33418/33418 [00:26<00:00, 1277.21it/s]\n",
      "[train epoch 1] loss: 2.571 ppl: 13.074 : 100%|█████████████████████████████████| 45100/45100 [02:55<00:00, 256.35it/s]\n",
      "[valid epoch 1] loss: 2.567 ppl: 13.031 : 100%|████████████████████████████████| 33418/33418 [00:26<00:00, 1265.73it/s]\n",
      "[train epoch 2] loss: 2.564 ppl: 12.986 : 100%|█████████████████████████████████| 45100/45100 [03:20<00:00, 225.37it/s]\n",
      "[valid epoch 2] loss: 2.556 ppl: 12.888 : 100%|████████████████████████████████| 33418/33418 [00:22<00:00, 1491.54it/s]\n",
      "[valid epoch 2] loss: 2.559 ppl: 12.923 : 100%|████████████████████████████████| 28570/28570 [00:16<00:00, 1690.81it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'NNLM_alpha': 11.20437723120408,\n",
       " 'ngram_alpha': 15.891757135551002,\n",
       " 'TextRnn_alpha': 13.595520055159893,\n",
       " 'TextRnnAttention_alpha': 12.92293164156136}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_alpha('NNLM_alpha')\n",
    "main_alpha('ngram_alpha')\n",
    "main_alpha('TextRnn_alpha')\n",
    "main_alpha('TextRnnAttention_alpha')\n",
    "test_loss_models_alpha\n",
    "test_ppl_models_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter letters: w h a t e\n",
      "Word completion： w h a t e d #\n",
      "Next letter candidate: ['d', 'r', '#', 's', 'n']\n"
     ]
    }
   ],
   "source": [
    "sentence = 'w h a t e'\n",
    "test_model = './TextRnnAttention_alpha/weights-{}.ckpt'.format(epochs)\n",
    "next_alpha_predict(test_model,sentence)\n",
    "rank_alpha_predict(test_model,sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter letters: s t u d e n\n",
      "Word completion： s t u d e n #\n",
      "Next letter candidate: ['#', 'a', 'e', 's', 't']\n"
     ]
    }
   ],
   "source": [
    "sentence = 's t u d e n'\n",
    "test_model = './TextRnnAttention_alpha/weights-{}.ckpt'.format(epochs)\n",
    "next_alpha_predict(test_model,sentence)\n",
    "rank_alpha_predict(test_model,sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter letters: w a c t h\n",
      "Word completion： w a c t h e d #\n",
      "Next letter candidate: ['e', 'i', 'a', 'n', 'o']\n"
     ]
    }
   ],
   "source": [
    "sentence = 'w a c t h'\n",
    "test_model = './TextRnnAttention_alpha/weights-{}.ckpt'.format(epochs)\n",
    "next_alpha_predict(test_model,sentence)\n",
    "rank_alpha_predict(test_model,sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocab_path = './vocab.json'\n",
    "word2index_dict, index2word_dict = generate_vocab(train_path)\n",
    "n_class = len(word2index_dict)  \n",
    "# instantiated dataset\n",
    "n_step = 5  \n",
    "emb_size = 128\n",
    "n_hidden = 5\n",
    "epochs = 1\n",
    "batch_size = 1\n",
    "nw = min([os.cpu_count(), batch_size if  batch_size > 1 else 0, 8]) \n",
    "train_dataset = generate_dataset(train_path, word2index_dict, n_step)\n",
    "valid_dataset = generate_dataset(valid_path, word2index_dict, n_step)\n",
    "test_dataset = generate_dataset(test_path, word2index_dict, n_step)\n",
    "train_loader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True, pin_memory=True,num_workers=nw)\n",
    "valid_loader = DataLoader(valid_dataset,batch_size=batch_size,shuffle=True,pin_memory=True,num_workers=nw)\n",
    "test_loader = DataLoader(test_dataset,batch_size=batch_size,shuffle=True,pin_memory=True,num_workers=nw)\n",
    "model_dic = {\"ngram\":n_gram(n_class,n_step,emb_size), \"TextRnn\":TextRNN(n_class,emb_size,n_hidden),  \"TextRnnAttention\":TextRNN_attention(n_class,emb_size,n_hidden),\"NNLM\":NNLM(n_class,n_step,emb_size,n_hidden)}\n",
    "mm1 = trainer(n_step,epochs,train_path,valid_path,test_path,train_loader,test_loader,valid_loader,word2index_dict,index2word_dict,model_dic)\n",
    "test_loss_models = {}\n",
    "test_ppl_models = {}\n",
    "def main_words(model_name):\n",
    "    models_path = './' + model_name\n",
    "    test_loss, test_ppl,trains_loss,valids_loss,trains_ppl,valids_ppl = mm1.train_test_model(model_name)\n",
    "    f_loss = open(models_path + '/trian_valid_loss.txt','w',encoding='utf-8')\n",
    "    \n",
    "    f_ppl = open(models_path + '/trian_valid_ppl.txt','w',encoding='utf-8')\n",
    "    \n",
    "    for loss_t,loss_v in zip(trains_loss,valids_loss):\n",
    "        f_loss.write(str(loss_t) + '\\t' + str(loss_v) + '\\n')\n",
    "        \n",
    "    for ppl_t,ppl_v in zip(trains_loss,valids_loss):\n",
    "        f_ppl.write(str(ppl_t) + '\\t' + str(ppl_v) + '\\n')\n",
    "        \n",
    "    test_loss_models[model_name] = test_loss\n",
    "    test_ppl_models[model_name] = test_ppl\n",
    "# next word prediction\n",
    "def test_next_word(test_model,sentence):\n",
    "    print('Enter a sentence: {}'.format(sentence))\n",
    "    model = torch.load(test_model).to(device)\n",
    "    with open(vocab_path, \"r\") as f:\n",
    "            word2index_dict = json.load(f)\n",
    "    index2word_dict = {index: word for word, index in word2index_dict.items()}\n",
    "    count = 0\n",
    "    while count < 10:\n",
    "        count += 1\n",
    "        sentence_dataset = generate_dataset_one(sentence,word2index_dict=word2index_dict)\n",
    "        data_loader = DataLoader(sentence_dataset,batch_size=1,shuffle=False,pin_memory=True,num_workers=nw)\n",
    "        for step, data in enumerate(data_loader):\n",
    "            input, target = data\n",
    "            pred = model(input.to(device))\n",
    "            preds = pred.data.max(1, keepdim=True)[1]\n",
    "            sentence = sentence + ' ' + index2word_dict[preds.item()]\n",
    "        if count == 10:\n",
    "            print('Predict the word ten times ahead：{}'.format(sentence))\n",
    "def rank_words_predict(test_model,sentence):\n",
    "    model = torch.load(test_model).to(device)\n",
    "    with open(vocab_path, \"r\") as f:\n",
    "            word2index_dict = json.load(f)\n",
    "    index2word_dict = {index: word for word, index in word2index_dict.items()}\n",
    "    count = 0\n",
    "    sentence_dataset = generate_dataset_one(sentence,word2index_dict=word2index_dict)\n",
    "    data_loader = DataLoader(sentence_dataset,batch_size=1,shuffle=False,pin_memory=True,num_workers=nw)\n",
    "    for step, data in enumerate(data_loader):\n",
    "        input, target = data\n",
    "        pred = model(input.to(device))\n",
    "        preds = pred.data.max(1, keepdim=True)[1]\n",
    "        rank_ids = torch.argsort(pred,dim=1,descending=True)[:,:5].numpy()[0]\n",
    "        rank_words = [index2word_dict[i] for i in rank_ids]\n",
    "        print('Next word candidate: {}'.format(rank_words))\n",
    "        sentence = sentence + ' ' + index2word_dict[preds.item()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train epoch 0] loss: 8.759 ppl: 6365.140 : 100%|██████████████████████████████| 38039/38039 [1:39:06<00:00,  6.40it/s]\n",
      "[valid epoch 0] loss: 9.120 ppl: 9137.179 : 100%|███████████████████████████████| 31780/31780 [01:34<00:00, 335.03it/s]\n",
      "[valid epoch 0] loss: 9.125 ppl: 9182.384 : 100%|███████████████████████████████| 25560/25560 [01:13<00:00, 346.89it/s]\n",
      "[train epoch 0] loss: 9.855 ppl: 19053.962 : 100%|███████████████████████████████| 38039/38039 [35:47<00:00, 17.71it/s]\n",
      "[valid epoch 0] loss: 11.557 ppl: 104501.943 : 100%|████████████████████████████| 31780/31780 [00:44<00:00, 719.28it/s]\n",
      "[valid epoch 0] loss: 11.672 ppl: 117185.032 : 100%|████████████████████████████| 25560/25560 [00:32<00:00, 789.80it/s]\n",
      "[train epoch 0] loss: 9.493 ppl: 13264.919 : 100%|███████████████████████████████| 38039/38039 [27:15<00:00, 23.26it/s]\n",
      "[valid epoch 0] loss: 11.495 ppl: 98206.987 : 100%|█████████████████████████████| 31780/31780 [00:51<00:00, 620.84it/s]\n",
      "[valid epoch 0] loss: 11.611 ppl: 110286.279 : 100%|████████████████████████████| 25560/25560 [00:40<00:00, 626.76it/s]\n",
      "[train epoch 0] loss: 9.468 ppl: 12941.417 : 100%|███████████████████████████████| 38039/38039 [24:20<00:00, 26.05it/s]\n",
      "[valid epoch 0] loss: 11.537 ppl: 102399.150 : 100%|████████████████████████████| 31780/31780 [00:37<00:00, 838.64it/s]\n",
      "[valid epoch 0] loss: 11.639 ppl: 113426.148 : 100%|████████████████████████████| 25560/25560 [00:29<00:00, 870.27it/s]\n"
     ]
    }
   ],
   "source": [
    "main_words('NNLM')\n",
    "main_words('ngram')\n",
    "main_words('TextRnnAttention')\n",
    "main_words('TextRnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a sentence: as the dow jones industrial it\n",
      "Predict the word ten times ahead：as the dow jones industrial it the the the the the the the the the the\n",
      "Next word candidate: ['the', 'of', 'and', 'in', 'to']\n"
     ]
    }
   ],
   "source": [
    "sentence = 'as the dow jones industrial it' \n",
    "test_model = './ngram/weights-{}.ckpt'.format(epochs)\n",
    "test_next_word(test_model,sentence)\n",
    "rank_words_predict(test_model,sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a sentence: a single Macchi crash landed near\n",
      "Predict the word ten times ahead：a single Macchi crash landed near the the the the the the the the the the\n",
      "Next word candidate: ['the', 'in', 'and', 'of', 'to']\n"
     ]
    }
   ],
   "source": [
    "# was commissioned by\n",
    "# a single Macchi crash landed near Zemun airfield\n",
    "\n",
    "sentence = 'a single Macchi crash landed near' \n",
    "test_model = './TextRnn/weights-{}.ckpt'.format(epochs)\n",
    "test_next_word(test_model,sentence)\n",
    "rank_words_predict(test_model,sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a sentence: was contacted by Maxim and was given a photo shoot that appeared\n",
      "Predict the word ten times ahead：was contacted by Maxim and was given a photo shoot that appeared the the the the the the the the the the\n",
      "Next word candidate: ['the', 'a', 'in', 'of', 'to']\n"
     ]
    }
   ],
   "source": [
    "#  was contacted by Maxim and was given a photo shoot that appeared in the men's magazine.\n",
    "sentence = 'was contacted by Maxim and was given a photo shoot that appeared' \n",
    "test_model = './ngram/weights-{}.ckpt'.format(epochs)\n",
    "test_next_word(test_model,sentence)\n",
    "rank_words_predict(test_model,sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a sentence: Nice to meet you\n",
      "Predict the word ten times ahead：Nice to meet you to He was the by saying, on the John and\n",
      "Next word candidate: ['to', 'with', 'after', 'which', 'the']\n"
     ]
    }
   ],
   "source": [
    "sentence = 'Nice to meet you' \n",
    "test_model = './NNLM/weights-{}.ckpt'.format(epochs)\n",
    "test_next_word(test_model,sentence)\n",
    "rank_words_predict(test_model,sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a sentence: Happy Birthday to\n",
      "Predict the word ten times ahead：Happy Birthday to be able to the enough of the latter I and\n",
      "Next word candidate: ['be', 'Creek', 'work', 'Notes', 'she']\n"
     ]
    }
   ],
   "source": [
    "sentence = 'Happy Birthday to' \n",
    "test_model = './NNLM/weights-{}.ckpt'.format(epochs)\n",
    "test_next_word(test_model,sentence)\n",
    "rank_words_predict(test_model,sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a sentence: I am glad that you\n",
      "Predict the word ten times ahead：I am glad that you the Her is and with the of the Stafford of\n",
      "Next word candidate: ['the', 'his', 'that', 'Stafford', 'not']\n"
     ]
    }
   ],
   "source": [
    "sentence = 'I am glad that you' \n",
    "test_model = './NNLM/weights-{}.ckpt'.format(epochs)\n",
    "test_next_word(test_model,sentence)\n",
    "rank_words_predict(test_model,sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
